{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 - Answers for Galaxy API\n",
    "===================================\n",
    "\n",
    "**Goal**: Upload a file to a new history, import a workflow and run it on the uploaded dataset.\n",
    "\n",
    "1) Define the connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from six.moves.urllib.parse import urljoin\n",
    "\n",
    "server = 'https://usegalaxy.eu/'\n",
    "api_key = ''\n",
    "base_url = urljoin(server, 'api')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Create a new Galaxy history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotation': None,\n",
      " 'contents_url': '/api/histories/2bb432f0766c6256/contents',\n",
      " 'create_time': '2020-07-16T00:25:04.032927',\n",
      " 'deleted': False,\n",
      " 'empty': True,\n",
      " 'genome_build': None,\n",
      " 'id': '2bb432f0766c6256',\n",
      " 'importable': False,\n",
      " 'model_class': 'History',\n",
      " 'name': 'New history',\n",
      " 'published': False,\n",
      " 'purged': False,\n",
      " 'size': 0,\n",
      " 'slug': None,\n",
      " 'state': 'new',\n",
      " 'state_details': {'discarded': 0,\n",
      "                   'empty': 0,\n",
      "                   'error': 0,\n",
      "                   'failed_metadata': 0,\n",
      "                   'new': 0,\n",
      "                   'ok': 0,\n",
      "                   'paused': 0,\n",
      "                   'queued': 0,\n",
      "                   'running': 0,\n",
      "                   'setting_metadata': 0,\n",
      "                   'upload': 0},\n",
      " 'state_ids': {'discarded': [],\n",
      "               'empty': [],\n",
      "               'error': [],\n",
      "               'failed_metadata': [],\n",
      "               'new': [],\n",
      "               'ok': [],\n",
      "               'paused': [],\n",
      "               'queued': [],\n",
      "               'running': [],\n",
      "               'setting_metadata': [],\n",
      "               'upload': []},\n",
      " 'tags': [],\n",
      " 'update_time': '2020-07-16T00:25:04.032940',\n",
      " 'url': '/api/histories/2bb432f0766c6256',\n",
      " 'user_id': '1c510fef372551ec',\n",
      " 'username_and_slug': None}\n"
     ]
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "data = {'name': 'New history'}\n",
    "r = requests.post(base_url + '/histories', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "new_hist = r.json()\n",
    "pprint(new_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Upload** the local file \"test-data/1.txt\" to a new dataset in the created history. You need to run the special 'upload1' tool by making a `POST` request to `/api/tools`. You don't need to pass any inputs to it apart from attaching the file as 'files_0|file_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'implicit_collections': [],\n",
      " 'jobs': [{'create_time': '2020-07-16T00:34:52.194529',\n",
      "           'exit_code': None,\n",
      "           'galaxy_version': '20.05',\n",
      "           'history_id': '2bb432f0766c6256',\n",
      "           'id': 'bbd44e69cb8906b58d211aca0c46ba7e',\n",
      "           'model_class': 'Job',\n",
      "           'state': 'new',\n",
      "           'tool_id': 'upload1',\n",
      "           'update_time': '2020-07-16T00:34:52.271800'}],\n",
      " 'output_collections': [],\n",
      " 'outputs': [{'create_time': '2020-07-16T00:34:52.077738',\n",
      "              'data_type': 'galaxy.datatypes.data.Data',\n",
      "              'deleted': False,\n",
      "              'file_ext': 'auto',\n",
      "              'file_size': 0,\n",
      "              'genome_build': '?',\n",
      "              'hda_ldda': 'hda',\n",
      "              'hid': 1,\n",
      "              'history_content_type': 'dataset',\n",
      "              'history_id': '2bb432f0766c6256',\n",
      "              'id': 'bbd44e69cb8906b57da5fc4426f3ca5c',\n",
      "              'metadata_dbkey': '?',\n",
      "              'misc_blurb': None,\n",
      "              'misc_info': None,\n",
      "              'model_class': 'HistoryDatasetAssociation',\n",
      "              'name': '1.txt',\n",
      "              'output_name': 'output0',\n",
      "              'peek': None,\n",
      "              'purged': False,\n",
      "              'state': 'queued',\n",
      "              'tags': [],\n",
      "              'update_time': '2020-07-16T00:34:52.162403',\n",
      "              'uuid': 'c738a347-ed0c-4500-bd63-1d08bc072972',\n",
      "              'validated_state': 'unknown',\n",
      "              'validated_state_message': None,\n",
      "              'visible': True}],\n",
      " 'produces_entry_points': False}\n"
     ]
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "data = {\n",
    "    'history_id': new_hist['id'],\n",
    "    'tool_id': 'upload1'}\n",
    "with open(\"test-data/1.txt\", 'rb') as f:\n",
    "    files = {'files_0|file_data': f}\n",
    "    r = requests.post(base_url + '/tools', data, params=params, files=files)\n",
    "ret = r.json()\n",
    "pprint(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Find the new uploaded dataset, either from the dict returned by the POST request or from the history contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_time': '2020-07-16T00:34:52.077738',\n",
      " 'data_type': 'galaxy.datatypes.data.Data',\n",
      " 'deleted': False,\n",
      " 'file_ext': 'auto',\n",
      " 'file_size': 0,\n",
      " 'genome_build': '?',\n",
      " 'hda_ldda': 'hda',\n",
      " 'hid': 1,\n",
      " 'history_content_type': 'dataset',\n",
      " 'history_id': '2bb432f0766c6256',\n",
      " 'id': 'bbd44e69cb8906b57da5fc4426f3ca5c',\n",
      " 'metadata_dbkey': '?',\n",
      " 'misc_blurb': None,\n",
      " 'misc_info': None,\n",
      " 'model_class': 'HistoryDatasetAssociation',\n",
      " 'name': '1.txt',\n",
      " 'output_name': 'output0',\n",
      " 'peek': None,\n",
      " 'purged': False,\n",
      " 'state': 'queued',\n",
      " 'tags': [],\n",
      " 'update_time': '2020-07-16T00:34:52.162403',\n",
      " 'uuid': 'c738a347-ed0c-4500-bd63-1d08bc072972',\n",
      " 'validated_state': 'unknown',\n",
      " 'validated_state_message': None,\n",
      " 'visible': True}\n"
     ]
    }
   ],
   "source": [
    "hda = ret['outputs'][0]\n",
    "pprint(hda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Import a workflow** from the local file \"test-data/convert_to_tab.ga\" by making a `POST` request to `/api/workflows`. The only needed data is 'workflow', which must be a deserialized JSON representation of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': [],\n",
      " 'create_time': '2020-07-16T00:35:45.726191',\n",
      " 'deleted': False,\n",
      " 'id': 'f0aea4ed75fe67a8',\n",
      " 'latest_workflow_uuid': '814e6545-db9e-4dc0-b9c4-1c0f9ed68910',\n",
      " 'model_class': 'StoredWorkflow',\n",
      " 'name': 'Convert to tab',\n",
      " 'number_of_steps': 2,\n",
      " 'owner': 'nsoranzo',\n",
      " 'published': False,\n",
      " 'tags': [],\n",
      " 'update_time': '2020-07-16T00:35:45.726210',\n",
      " 'url': '/api/workflows/f0aea4ed75fe67a8'}\n"
     ]
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "with open('test-data/convert_to_tab.ga', 'r') as f:\n",
    "    workflow_json = json.load(f)\n",
    "data = {'workflow': workflow_json}\n",
    "r = requests.post(base_url + '/workflows', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "wf = r.json()\n",
    "pprint(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) View the details of the imported workflow by making a GET request to `/api/workflows`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotation': None,\n",
      " 'create_time': '2020-07-16T00:35:45.726191',\n",
      " 'deleted': False,\n",
      " 'id': 'f0aea4ed75fe67a8',\n",
      " 'inputs': {'0': {'label': 'Input Dataset',\n",
      "                  'uuid': '671bca4e-0b76-4a6f-a0a2-70219df56576',\n",
      "                  'value': ''}},\n",
      " 'latest_workflow_uuid': '814e6545-db9e-4dc0-b9c4-1c0f9ed68910',\n",
      " 'model_class': 'StoredWorkflow',\n",
      " 'name': 'Convert to tab',\n",
      " 'owner': 'nsoranzo',\n",
      " 'published': False,\n",
      " 'steps': {'0': {'annotation': None,\n",
      "                 'id': 0,\n",
      "                 'input_steps': {},\n",
      "                 'tool_id': None,\n",
      "                 'tool_inputs': {'optional': False},\n",
      "                 'tool_version': None,\n",
      "                 'type': 'data_input'},\n",
      "           '1': {'annotation': None,\n",
      "                 'id': 1,\n",
      "                 'input_steps': {'input': {'source_step': 0,\n",
      "                                           'step_output': 'output'}},\n",
      "                 'tool_id': 'Convert characters1',\n",
      "                 'tool_inputs': {'__page__': 0,\n",
      "                                 '__rerun_remap_job_id__': None,\n",
      "                                 'condense': 'true',\n",
      "                                 'convert_from': 's',\n",
      "                                 'input': None,\n",
      "                                 'strip': 'true'},\n",
      "                 'tool_version': '1.0.0',\n",
      "                 'type': 'tool'}},\n",
      " 'tags': [],\n",
      " 'update_time': '2020-07-16T00:35:45.726210',\n",
      " 'url': '/api/workflows/f0aea4ed75fe67a8',\n",
      " 'version': 0}\n"
     ]
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "r = requests.get(base_url + '/workflows/' + wf['id'], params)\n",
    "wf = r.json()\n",
    "pprint(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) **Run** the imported workflow on the uploaded dataset **inside the same history** by making a `POST` request to `/api/workflows/<id>/invocations`. The only needed data are 'history' and 'inputs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'create_time': '2020-07-16T00:45:32.115437',\n",
      " 'history_id': '2bb432f0766c6256',\n",
      " 'id': '5dbd633d4749cb27',\n",
      " 'model_class': 'WorkflowInvocation',\n",
      " 'state': 'new',\n",
      " 'update_time': '2020-07-16T00:45:32.115450',\n",
      " 'uuid': 'a71edfe0-c6fd-11ea-b845-005056ba2773',\n",
      " 'workflow_id': '499e3c7b8a0ad7ac'}\n"
     ]
    }
   ],
   "source": [
    "params = {'key': api_key}\n",
    "inputs = {0: {'id': hda['id'], 'src': 'hda'}}\n",
    "data = {\n",
    "    'history': 'hist_id=' + new_hist['id'],\n",
    "    'inputs': inputs}\n",
    "r = requests.post(base_url + '/workflows/' + wf['id'] + '/invocations', json.dumps(data), params=params, headers={'Content-Type': 'application/json'})\n",
    "pprint(r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) View the results on the Galaxy server with your web browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
